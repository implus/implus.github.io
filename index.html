<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiang Li</title>
    <meta content="Xiang Li, https://implus.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        /* 添加模态框相关样式 */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            position: relative;
            background-color: transparent;
            margin: auto;
            display: flex;
            flex-direction: column;
            align-items: center;
            max-width: 90%;
            max-height: 90%;
        }

        .modal-img {
            max-width: 100%;
            max-height: 80vh;
            object-fit: contain;
            transition: transform 0.3s ease;
        }

        .close {
            position: absolute;
            top: -40px;
            right: 0;
            color: white;
            font-size: 35px;
            font-weight: bold;
            cursor: pointer;
        }

        .zoom-controls {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: flex;
            gap: 10px;
            z-index: 1001;
        }

        .zoom-btn {
            background-color: rgba(255, 255, 255, 0.8);
            border: none;
            border-radius: 4px;
            padding: 5px 10px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
        }

        .zoom-btn:hover {
            background-color: rgba(255, 255, 255, 1);
        }

        /* 为Honors Gallery中的图片添加鼠标样式 */
        .honors-gallery-img {
            cursor: pointer;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
        
        /* 导航栏样式 */
        .nav-sidebar {
            position: fixed;
            left: 20px;
            top: 20px;
            background-color: #fff;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            z-index: 1000;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .nav-sidebar ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        
        .nav-sidebar li {
            margin: 10px 0;
            padding: 0;
        }
        
        .nav-sidebar a {
            display: block;
            padding: 5px 10px;
            color: #1772d0;
            text-decoration: none;
            font-weight: bold;
            border-radius: 3px;
            transition: background-color 0.3s;
        }
        
        .nav-sidebar a:hover {
            background-color: #f0f0f0;
            color: #f09228;
        }
        
        /* 平滑滚动效果 */
        html {
            scroll-behavior: smooth;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<!-- 左侧导航栏 -->
<div class="nav-sidebar">
    <ul>
        <li><a href="#about-me">About Me</a></li>
        <li><a href="#students">Team</a></li>
        <li><a href="#confpapers">Honor</a></li>
        <li><a href="#honors_gallery">Honors Gallery</a></li>
        <li><a href="#experience">News</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#review">Review Services</a></li>
    </ul>
</div>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/me.png">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Xiang Li (李翔)</span><br>
            <span> <a href="https://cc.nankai.edu.cn/2021/0323/c13620a490349/page.htm">Associate Professor, College of Computer Science, Nankai University</a></span><br>
            <span><strong>Address</strong>: No. 38, Tongyan Road, Haihe Education Park, Tianjin, China</span><br>
            <span><strong>Email</strong>: xiang.li.implus [at] {nankai.edu.cn} </span> <br>
	    <span><strong>Research Group Page</strong>: <a href="https://github.com/IMPlus-PCALab">IMPlus@PCALab</a> </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2 id="about-me">About Me [<a href="https://github.com/implus">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ&hl=zh-CN">Google Scholar</a>]
	    [<a href="https://github.com/IMPlus-PCALab">Research Group</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->
        </h2>
        <div class="paper">
            李翔，南开大学副教授，博导，南开百青、五四青年奖章获得者，入选博新计划A档，获CCF优博提名，吴文俊人工智能优秀青年奖，斯坦福全球2%顶尖科学家。在CCF-A类会议及期刊上发表40余篇论文，谷歌学术引用1.7万余次，其中2篇一作论文单篇引用分别达3000和1500余次。相关工作被诺贝尔物理学奖、图灵奖得主Hinton教授团队重点跟进，并成为工业界主流轻量目标检测器 YOLO 系列的标准配置，获高水平期刊CVMJ 年度最佳论文提名奖。在全球大数据人工智能领域顶尖赛事中持续多年带领团队斩获多项冠军及高名次奖项，竞赛累计获奖金额达130余万，曾获计图Jittor人工智能挑战赛冠军（1/154队伍），滴滴研究院Di-Tech首届算法大赛全球总冠军（1/7664队伍），阿里巴巴天池大数据竞赛首届阿里移动推荐算法冠军（1/7186队伍），央视科教频道《走近科学》专集报道了这一成果。近年来带领团队开辟了“智能科研服务”研究领域，旨在利用多模态大模型技术解决科研人员的科研效率问题。部分成果以自动化科研资讯形式落地科研社区服务“减论”账号，半年累计产生了100万余次学习访问量，收获大量科研社区好评。减论IP创始人，新芽计划发起人，2025年度CCF学生领航计划（CCF SPP）工作组委员。
        </div>
        <div class="paper">
            I'm an <a href="https://cc.nankai.edu.cn/2021/0323/c13620a490349/page.htm">Associate Professor</a> in College of Computer Science, Nankai University, in the Team of <a href="https://mmcheng.net/cmm/">Ming-Ming Cheng</a>. 
            I got my PhD degree from the Department of Computer Science and Technology, Nanjing University of Science and Technology (NJUST) in 2020.            
	    My advisor is <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Prof. Jian Yang</a> from NJUST, who is a Changjiang Scholar. My vice-advisor is <a href="http://www.xlhu.cn/">Prof. Xiaolin Hu</a> from Tsinghua University.
	    I started my postdoctoral career in NJUST as a candidate for the <a href="https://zhuanlan.zhihu.com/p/147471409">2020 Postdoctoral Innovative Talent Program</a>, supervised by <a href="https://imag-njust.net/jinhui-tang/">Prof. Jinhui Tang</a>.
	    In 2016, I spent 8 months as a research intern in Microsoft Research Asia, supervised by <a href="https://scholar.google.com/citations?user=Bl4SRU0AAAAJ&hl=zh-CN">Prof. Tao Qin</a> and <a href="https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=zh-CN">Prof. Tie-Yan Liu</a>.
            I was a visiting scholar at <a href="https://www.momenta.cn/">Momenta</a>, mainly focusing on monocular perception algorithm.
            <br><br>

            My recent works are mainly on:
	    <ul>
        <li>Intelligent Research Service</li>
        <li>LLM Agent</li>
		<li>neural architecture design, CNN/Transformer</li>
		<li>object detection/recognition</li>
		<li>unsupervised learning</li>
		<li>knowledge distillation</li>
	    </ul>
            <p style='color:red'><strong>
                 We are looking for self-motivated students! Please feel free to contact me through the email (attach your CV).
		         We would not push, but you should always be self-driven for your own target, i.e., making solid and impactful contributions to the CV/AI community.
            </strong></p> 
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="students">Team</h2>
        <div style="display: flex; flex-wrap: wrap; justify-content: space-between; margin-left: 30px; margin-top: 30px;">
            <!-- 副教授组 -->
            <h3 style="width: 100%; margin-bottom: 15px;">副教授</h3>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/戴一冕.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">戴一冕</h3>
                <p><strong>状态:</strong> 副教授 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">红外小目标</span> </p>
                <p><strong>成果:</strong> 主持发布 SIRST V1、SIRST V2、DenseSIRST、HazyDet、GrokLST 等多个开源数据集。主持国自然青基、博士后面上等校企合作项目 5 项，主要成果发表在 IJCV、IEEE TGRS等国际知名期刊。谷歌学术引用 3500 余次引用，入选斯坦福前 2% 顶尖科学家榜单。曾获河南省教育厅科技成果二等奖（排名第二）、首届粤港澳大湾区国际算法算例大赛遥感目标检测赛道亚军、“吉林一号”杯卫星遥感应用青年创新创业大赛一等奖。 </p>
            </div>

            <hr style="width: 100%; margin: 20px 0; border: 0; border-top: 1px solid #ddd;">

            <!-- 博士组 -->
            <h3 style="width: 100%; margin-bottom: 15px;">博士</h3>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/杨凌风.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">杨凌风</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多模态感知，大模型</span> </p>
                <p><strong>成果:</strong>2021首届“征图杯”校园机器视觉人工智能大赛亚军，2022第二届计图人工智能挑战赛冠军，2022第五届开源创新大赛团体一等奖。在CVPR，NeurIPS，TPAMI 等顶级会议期刊上发表论文数篇，Google Scholar引用500+。影石Insta360自动剪辑算法研发主要成员，申请两项发明专利。2024院长奖章、优秀毕业生、博士生国家奖学金和优秀研究生干部获得者。</p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/赵鹏海.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">赵鹏海</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">智能科研服务</span> </p>
                <p><strong>成果:</strong>“减论Agent系统”、“减论APP”算法研发负责人，推动科研流程的智能化与自动化，减论IP学术账号获得全网3万+关注，播放量近100万次，HuggingFace接口调用5000余次。获得粤港澳国际算法算例大赛三等奖，在AAAI等国际顶级会议及SCI期刊上发表多篇学术论文。未来将持续探索智能技术在科研流程中的深度融合与应用创新。</p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/李宇轩.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李宇轩</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">遥感感知、大模型</span> </p>
                <p><strong>成果:</strong> 2022第二届计图人工智能挑战赛冠军，2022第五届开源创新大赛团体一等奖，2022首届粤港澳大湾区国际算法算例大赛二等奖。在NeurIPS，IJCV，ICCV等顶级会议期刊上发表论文数篇，Google Scholar引用1300+。2024 PRCV竞赛和2025 第七届全球校园人工智能算法精英大赛出题人。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/陈震元.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">陈震元</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多智能体感知、大模型</span> </p>
                <p><strong>成果:</strong> 南开-新奥质信实验室算法架构负责人，影石Insta360自动剪辑算法研发主要成员。多次获得CVPR2020、CVPR2021不完备数据竞赛语义分割项目和目标定位项目冠军、季军。于2023年获得第三届计图挑战赛季军和第六届开源设计大赛二等奖。曾于京东探索研究院和旷视科技进行实习。申请一项PCT国际专利和两项国内专利。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/李政.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李政</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多模态模型，模型压缩</span> </p>
                <p><strong>成果:</strong> 在Kaggle竞赛中获得两次金牌，获Kaggle Master。一作在ICCV，CVPR，AAAI等会议期刊上发表多篇论文，谷歌学术引用450+。曾获研究生国家奖学金。曾在旷视科技，蚂蚁集团，阿里巴巴达摩院进行研究性实习。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/武戈.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">武戈</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多模态模型，模型压缩</span> </p>
                <p><strong>成果:</strong> 2022年首届粤港澳大湾区国际算法算例大赛三等奖、2023年获得第三届计图挑战赛季军和第六届开源设计大赛二等奖。一作发表发表ECCV论文一篇。本科曾获河南省三好学生。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/唐文浩.png" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">唐文浩</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">智能科学</span> </p>
                <p><strong>成果:</strong> 专注于高分辨率图像分析，发表以下论文：路面病害识别：2022 ACM MM，2021, 2023 IEEE T-ITS; 计算病理学：2023 ICCV, 2024 CVPR。在硕士期间获得国家奖学金。</p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/吴俐伽.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">吴俐伽</h3>
                <p><strong>状态：</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">大模型、智能科研服务</span> </p>
                <p><strong>成果:</strong> “减论”APP后端研发核心团队；2024ICPC成都/沈阳银奖、2024ICPC东亚赛区决赛铜奖、2023ICPC杭州银奖、2024CCPC重庆银奖、2023CCPC深圳/秦皇岛银奖； </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/李林一.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李林一</h3>
                <p><strong>状态：</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">大模型、智能体</span> </p>
                <p><strong>成果:</strong> 南开-新奥质信实验室子课题算法负责人、2023ICPC西安邀请赛银奖、蓝桥杯国一、程序设计天梯赛国一 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/彭晨旭.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">彭晨旭</h3>
                <p><strong>状态：</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">遥感感知；交互式模型</span> </p>
                <p><strong>成果:</strong> 获Kaggle2金3银1铜；2021 CCF BDCI婴儿超声血管瘤分割冠军; 2022百度时序动作定位大赛冠军; 2023科大讯飞PET图像分析和疾病预测竞赛冠军; 2025 CVPR第四届反无人机竞赛赛道一冠军; 2024 ICPR弱监督红外小目标检测冠军; 2024 ICPR轻量级红外小目标检测冠军; 2024 PRCV广域红外小目标检测冠军; 2024长光卫星高分辨率道路提取竞赛一等奖; 2017江苏省高等数学竞赛一等奖；2019全国大学生数学建模竞赛二等奖。在NeuroImage，Medical Physics，PRCV等会议期刊上发表论文数篇，Google Scholar引用100+。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/王晨旭.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">王晨旭</h3>
                <p><strong>状态:</strong> 博士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">遥感感知、半监督学习</span> </p>
                <p><strong>成果:</strong> 专注于遥感目标检测领域，2025 CVPR第四届反无人机竞赛赛道一冠军赛道二亚军，2024长光卫星高分辨率道路提取竞赛三等奖；一作发表AAAI 2025论文一篇，二作发表NeurIPS2024论文一篇 </p>
            </div>
            

            <hr style="width: 100%; margin: 20px 0; border: 0; border-top: 1px solid #ddd;">

            <!-- 硕士组 -->
            <h3 style="width: 100%; margin-bottom: 15px;">硕士</h3>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/张鑫.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">张鑫</h3>
                <p><strong>状态:</strong> 硕士</p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">计算机视觉、遥感感知</span> </p>
                <p><strong>成果:</strong> 获首届粤港澳大湾区国际算法算例大赛亚军、2023计图人工智能大赛三等奖、2024ISPRS遥感图像解译大赛亚军、2025 长光"吉林一号"杯遥感应用大赛冠军。一作发表CVPR论文一篇，共一发表ECCV论文一篇。本科曾获国家奖学金、重庆市优秀毕业论文、重庆大学十佳优秀共青团员。 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/李丹阳.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李丹阳</h3>
                <p><strong>状态:</strong> 硕士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">大模型推理分割，遥感变化检测</span> </p>
                <p><strong>成果:</strong> 曾获：2024 ISPRS多模态遥感应用算法解译大赛冠军、2025 长光"吉林一号"杯遥感应用大赛冠军、2025 CVPR第四届反无人机竞赛赛道一冠军赛道二亚军、国家奖学金(本科)等荣誉</p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/庞天傲.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">庞天傲</h3>
                <p><strong>状态:</strong> 硕士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论后端算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 全国大学生数学建模竞赛天津赛区二等奖，25届考研分数410+ </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/过翔天.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">过翔天</h3>
                <p><strong>状态:</strong> 硕士 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多模态模型、遥感感知</span> </p>
                <p><strong>成果:</strong> 2022台达杯国际太阳能竞赛优秀奖，25届考研分数410+ </p>
            </div>

            <hr style="width: 100%; margin: 20px 0; border: 0; border-top: 1px solid #ddd;">

            <!-- 本科组 -->
            <h3 style="width: 100%; margin-bottom: 15px;">本科</h3>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/章壹程.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">章壹程</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">遥感感知、大模型</span> </p>
                <p><strong>成果:</strong> 2024ICPC成都/杭州银奖、2024数学建模比赛天津市二等奖 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/刘砚桐.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">刘砚桐</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论后端、智能科研服务</span> </p>
                <p><strong>成果:</strong> 2024ICPC成都/杭州银奖、2024ICPC西安邀请赛金奖、2025 长光"吉林一号"杯遥感应用大赛冠军 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/邢清画.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">邢清画</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">大模型、智能科研服务</span> </p>
                <p><strong>成果:</strong> 获得南开大学国家励志奖学金，在AAAI2025以第二作者发表论文一篇，参与减论agent部分算法的开发和优化 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/田晋宇.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">田晋宇</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">大模型、智能科研服务</span> </p>
                <p><strong>成果:</strong> 获得南开大学国家励志奖学金，在AAAI2025以第四作者发表论文一篇，参与减论agent部分算法的开发和优化 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/周重天.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">周重天</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论后端、智能科研服务</span> </p>
                <p><strong>成果:</strong> 参与减论APP后端算法的开发和优化 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/杨峥芃.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">杨峥芃</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 第二届“吉林一号”杯卫星遥感应用青年创新创业大赛 赛题D三等奖，互联网+减论项目主要负责人 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/王雨萌.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">王雨萌</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 获得南开大学公能奖学金，2024全国大学生数学建模竞赛天津赛区二等奖，南开火山杯减论算法负责人 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/李政霖.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李政霖</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 减论产品设计社区板块负责人、减论基金申请 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/郭鑫隆.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">郭鑫隆</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 获得南开大学学业优秀奖学金，减论产品经理 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/李颖贤.png" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李颖贤</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">多智能体感知、大模型</span> </p>
                <p><strong>成果:</strong> 获得南开大学国家奖学金，参与POI算法研究和AVT相关工作 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/李昱.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">李昱</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 减论产品经理及UI设计、减论基金申请 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/刘祥宇.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">刘祥宇</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 减论产品经理及我的板块设计 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/钱俊玮.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">钱俊玮</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 减论平台作者定位与引用评价研发负责人 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/陶文烁.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">陶文烁</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 南开大学物理学术竞赛作品赛一等奖、互联网+减论项目主要负责人 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/向宇涵.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">向宇涵</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 获得南开大学公能奖学金，2023全国大学生数学建模竞赛天津赛区一等奖，参与基础数据元信息提取相关工作 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/张耕嘉.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">张耕嘉</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 南开大学公能奖学金，2023美国大学生数学建模竞赛H奖，2024全国大学生数学建模竞赛天津赛区省级二等奖，PolarDB数据库创新设计赛优胜奖 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/许洋.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">许洋</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论算法、智能科研服务</span> </p>
                <p><strong>成果:</strong> 南开大学公能奖学金 </p>
            </div>
            <div style="width: 30%; margin-bottom: 20px; text-align: center;">
                <img src="./resources/student/本科组/朱佳慧.jpg" style="width: 180px; height: 180px; object-fit: cover; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);">
                <h3 style="margin-top: 10px; margin-bottom: 5px;">朱佳慧</h3>
                <p><strong>状态:</strong> 本科 </p>
                <p><strong>研究方向:</strong> <span style="color: #1772d0;">减论产品、智能科研服务</span> </p>
                <p><strong>成果:</strong> 第二届“吉林一号”杯卫星遥感应用青年创新创业大赛 赛题D三等奖 </p>
            </div>

        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
	(See more details (codes, solutions, summaries) in <a href="https://github.com/IMPlus-PCALab/AICompetition">[AICompetition of Group Page]</a>)
        <div class="paper">
            <ul>
        <li>
            团队获CVPR workshop即The 4th Anti-UAV Workshop & Challenge<strong>最佳论文奖</strong>、Track 1赛道<strong>第一名</strong>、Track 2赛道<strong>第二名</strong>
        </li>
        <li>
            团队本科生获2025年南开大学“火山杯”AI智能体创新应用大赛软件专业组<a href="https://mp.weixin.qq.com/s/p8oRVXLIlfdwwwnkwYtpHA"><strong>一等奖</a>, <strong>5,000 RMB bonus</strong>
        </li>
        <li>
            团队获2025年第二届“吉林一号”杯长光卫星遥感应用青年创新创业大赛<a href="https://mp.weixin.qq.com/s/ZUeLj20NYB_icv18I4JH0w"><strong>特等奖、一等奖</strong>、三等奖，</a> <strong>24,000 RMB bonus</strong>
        </li>
		<li>
		    1st place of of Change Detection in High-resolution and Multi-temporal Optical Images, 2nd place of Forgery Detection in Multi-scenario Remote Sensing Images of Typical Objects in 2024 TC I Contest on <a href="https://mp.weixin.qq.com/s/tnx1cWeyt3siIjBD0civsQ">Intelligent Interpretation for Multi-modal Remote Sensing Application</a>, total <strong>13,000 RMB bonus</strong>
		</li>
		<li>
		    Second place of IACC International Algorithm Case Competition, namely the <a href="https://www.cvmart.net/race/10345/base">remote sensing detection</a>,  <strong>100,000 RMB bonus (2nd from 116 teams)</strong>
		</li>
		<li>
                    Champion of 2022 Jittor AI competition, namely the <a href="https://www.educoder.net/competitions/index/Jittor-3">landscape picture generation</a>, <strong>50,000 RMB bonus (1st from 154 teams)</strong>
                </li>
                <li>
                    Second place of 2020 Zhengtu Cup's first AI competition, namely the industrial defect detection algorithm, <strong>150,000 RMB bonus (2nd from 900 teams)</strong>
                </li>
                <li>
                    Champion of 2016 Didi Tech Di-Tech's first big data competition, namely the travel demand prediction algorithm, <strong>100,000 US dollars bonus (1st from 7664 team)</strong>
                </li>
                <li>
                    Champion of 2015 Alibaba Tianchi's first big data competition, namely Ali mobile recommendation algorithm, <strong>300,000 RMB bonus (1st from 7186 team)</strong> 
                </li>
                <li>
                    2015 Dean Medal of School of Computer Science, Nanjing University of Science and Technology, 2016 Presidential Medal of Nanjing University of Science and Technology, 2016 National Scholarship
                </li>
                <li>
                    ACM-ICPC Asia Regional Contest, Silver Medal (1st)
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="honors_gallery">Honors Gallery</h2>
        <div style="margin-top: 20px;">
            <!-- 优秀毕业论文展示区 -->
            <h3 style="margin-bottom: 15px;">优秀毕业论文</h3>
            <div style="display: flex; flex-wrap: wrap; justify-content: center; margin-bottom: 30px;">
                <div style="margin: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/优秀毕业论文_张鑫_重庆市.jpg" style="max-width: 320px; max-height: 240px; object-fit: contain;">
                    </div>
                    <div style="padding: 10px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 14px;">张鑫 - 重庆市优秀毕业论文</p>
                    </div>
                </div>
                <div style="margin: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/优秀毕业论文_高森森_南开大学.jpg" style="max-width: 320px; max-height: 240px; object-fit: contain;">
                    </div>
                    <div style="padding: 10px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 14px;">高森森 - 南开大学优秀毕业论文</p>
                    </div>
                </div>
            </div>
            
            <!-- 其他荣誉证书展示区 -->
            <h3 style="margin-bottom: 15px;">竞赛获奖证书</h3>
            <div style="display: flex; flex-wrap: wrap; justify-content: center;">
                <!-- 第一行 -->
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2025CVPRW_1st_place_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">CVPRW一等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2025CVPRW_best_paper_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">CVPRW最佳论文奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2025CVPRW_2nd_place_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">CVPRW二等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2025火山杯.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">火山杯一等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/长光卫星_Chainey_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">长光卫星杯一等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/长光卫星_ReductChange_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">长光卫星杯特等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2024年ISPRS第一技术委员会多模态遥感应用算法智能解译大赛_基于高分辨率可见光图像的感兴趣区域内部变化智能检测_赛道冠军_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">ISPRS变化检测赛道冠军</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2024年ISPRS第一技术委员会多模态遥感应用算法智能解译大赛_面向多场景的典型目标遥感图像真伪鉴别_赛道亚军_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">ISPRS真伪鉴别赛道亚军</p>
                    </div>
                </div>
                <!-- 第二行 -->
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/7_2022计图人工智能挑战赛冠军.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2022计图人工智能挑战赛冠军</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/8_2022粤港澳大湾区（黄埔）国际算法算例大赛遥感目标检测赛道亚军.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2022粤港澳大湾区算法大赛亚军</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/9_2020征途杯校园机器视觉人工智能大赛亚军.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2020征途杯校园机器视觉大赛亚军</p>
                    </div>
                </div>
                <!-- 第三行 -->
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/5_2015阿里巴巴大数据竞赛冠军.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2015阿里巴巴大数据竞赛冠军</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/6_2016滴滴研究院大数据竞赛冠军.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2016滴滴研究院大数据竞赛冠军</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/第五届开源创新大赛-开源任务挑战赛团体一等奖.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">第五届开源创新大赛团体一等奖</p>
                    </div>
                </div>
                <div style="margin: 6px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 5px; overflow: hidden; transition: transform 0.3s; width: 22%;" onmouseover="this.style.transform='scale(1.03)'" onmouseout="this.style.transform='scale(1)'">
                    <div style="display: flex; justify-content: center; align-items: center;">
                        <img src="./resources/honors/2023计图挑战赛二等奖_00.jpg" style="max-width: 240px; max-height: 200px; object-fit: contain;">
                    </div>
                    <div style="padding: 8px; text-align: center; background-color: #ffffff;">
                        <p style="margin: 0; font-size: 12px;">2023计图挑战赛二等奖</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
        <li>
            2025-06-26: 2 papers accepted in ICCV 2025, including <a href="https://zhengli97.github.io/ATPrompt/">ATPrompt</a>, an attribute-guided prompt learning approach!
        </li>
        <li>
            2025-04-09: I have been honored as a recipient of Nankai University's 2025 <a href="https://mp.weixin.qq.com/s/sGItgC7x-BAXAfbnh3-Ffw">May Fourth Youth Medal</a>.
        </li>
        <li>
            2025-02-27: 2 papers accepted in CVPR 2025, including <a href="https://github.com/zhasion/RSAR">RSAR</a>, a sota Restricted State Angle Resolver and Rotated SAR Benchmark!
        </li>
		<li>
			2024-10-07: <a href="https://arxiv.org/pdf/2403.11735">LSKNet: A foundation lightweight backbone for remote sensing</a> published in <a href="https://link.springer.com/article/10.1007/s11263-024-02247-9?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20241007&utm_content=10.1007%2Fs11263-024-02247-9">IJCV</a>.
		</li>
		<li>
			2024-09-26: 3 papers accepted in NeurIPS 2024, including <a href="https://github.com/zcablii/SARDet_100K">SARDet100K</a>.
		</li>
		<li>
			2024-09-16: I was selected for the 2024 World’s Top 2% Scientists list released by Stanford University, USA. See details <a href="https://mp.weixin.qq.com/s/eJIK5x-nEzy07Ltz5Ypi3g">here</a>.
		</li>
		<li>
			2024-07-02: 2 papers accepted in ECCV 2024.
		</li>
		<li>
			2024-06-22: JianLun (减论) IP Project is officially started, focusing on efficient AI understanding and education.
		</li>
		<li>
			2024-05-29: 1 paper <a href="https://github.com/Zzh-tju/ZoneEval">Zone Evaluation</a> accepted in TPAMI.
		</li>
		<li>
			2024-03-07: <a href="http://www.wuwenjunkejijiang.cn/a/2237.html">Final round</a> of The Wu Wenjun AI Outstanding Youth Award.
		</li>
		<li>
			2024-02-27: 2 papers (<a href="https://github.com/zhengli97/PromptKD">PromptKD</a>, <a href="https://github.com/jbwang1997/CrossKD">CrossKD</a>) accepted in CVPR 2024.
		</li>
		<li>
			2023-09-23: 1 paper (<a href="https://openreview.net/pdf?id=l6R4Go3noz">FGVP</a>) accepted in NeurIPS 2023.
		</li>
		<li>
			2023-07-14: 3 papers (including <a href="https://arxiv.org/abs/2303.09030.pdf">LSKNet</a>, ADNet) accepted in ICCV 2023. 
		</li>
		<li>
			2023-04-25: 1 paper <a href="">DUAL for Panoramic Depth Completion</a> accepted in ICML 2023. 
		</li>
		<li>    
			2023-01: Nomination Award for the CCF Excellent Doctoral Dissertation Incentive Plan. See <a href="https://www.ccf.org.cn/Awards/Awards/2022-12-08/781242.shtml">first round</a>, <a href="https://www.ccf.org.cn/Awards/Awards/2023-01-04/783561.shtml">final result</a>. 
		</li>
		<li> 
			2022-11-19: 4 papers (including <a href="https://arxiv.org/pdf/2211.16231.pdf">Curriculum Temperature</a>, <a href="https://arxiv.org/pdf/2211.10994.pdf">DesNet</a>) accepted in AAAI 2023. 
		</li>
		<li> 
			2022-09-15: 2 papers (<a href="https://arxiv.org/pdf/2203.06844.pdf">RecursiveMix</a>, <a href="https://arxiv.org/pdf/2207.05536.pdf">DTG-SSOD</a>) accepted in NeurIPS 2022. 
		</li>
	     	<li> 
			2022-07-05: 3 papers (<a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet</a>, <a href="https://arxiv.org/pdf/2203.09855.pdf">M3PT</a>, <a href="https://arxiv.org/pdf/2203.16317.pdf">PseCo</a>) accepted in ECCV 2022. 
		</li>
	     	<li> 
			2022-05-20: 1 paper (<a href="https://arxiv.org/pdf/2205.10063.pdf">UM-MAE</a>) is publicly available in arXiv. 
		</li>
	     	<li> 
			2022-03-02: 1 paper (<a href="https://arxiv.org/pdf/2203.03253.pdf">dynamicMLP</a>) accepted (oral) in CVPR 2022. 
		</li>
	     	<li> 
			2021-12-01: 1 paper (<a href="https://arxiv.org/pdf/2112.04840.pdf">KD for object detection</a>) accepted in AAAI 2022. 
		</li>
		<li>
                    	2021-05-05: 1 paper (<a href="https://ieeexplore.ieee.org/document/9423611">PAN++</a>) is accepted by TPAMI 2021. 
                </li>
                <li> 
			2021-03-01: 1 paper (<a href="https://arxiv.org/pdf/2011.12885.pdf">GFocalv2</a>) accepted in CVPR 2021.  
		</li>
                <li> 	
			2020-09-25: 1 paper (<a href="https://arxiv.org/pdf/2006.04388.pdf">GFocal</a>) accepted in NeurIPS 2020.  
		</li>
                <li> 	2019-12-01: 1 paper (<a href="https://ojs.aaai.org/index.php/AAAI/article/download/5904/5760">Understanding the disharmony v2</a>) accepted in AAAI 2020.  </li>
                <li> 	2019-03-15: 3 papers (<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">SKNet</a>,
			<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">Understanding the disharmony v1</a>,
			<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">PSENet</a>) accepted in CVPR 2019. </li>
		<li> 	2020-03-01: 1 paper (<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf">ST-CGAN</a>) accepted in CVPR 2018.  </li>
                <li> 	2020-06-16: 1 paper (<a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">MixNet</a>) accepted in IJCAI 2018. </li>
		<li>    2016-09-30: 1 paper (<a href="https://arxiv.org/pdf/1610.09893.pdf">LightRNN</a>) accepted in NeurIPS 2016. </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    


<div style="clear: both;">
    <div class="section">
        <h2 id="publications">Selected Publications</h2>
	(* indicates equal contribution, # corresponding author)

    <div class="paper"><img class="paper" src="./resources/paper_icon/arxiv_2025_REG.png" title="Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think.">
        <div><strong>Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think.</strong>
            <br>Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, Xiang Li#  <br>in arXiv, 2025<br>
            <a href="https://arxiv.org/abs/2507.01467">[Paper]</a>
            <a href="./resources/bibtex/arXiv_2025_REG.txt">[BibTex]</a>
            <a href="https://github.com/Martinser/REG?tab=readme-ov-file">[Code]</a><img src="https://img.shields.io/github/stars/Martinser/REG?style=social"/>
            <a href="https://mp.weixin.qq.com/s/BwKIY1r0o8Gdz55c7SiV1Q">[中文解读]</a>
            <br>
            <alert>
            REG is a simple yet effective method that entangles low-level image latents with a single high-level class token derived from pretrained foundation models for denoising. REG significantly improves generation quality and training convergence efficiency.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>


		
	<div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2025_ATPrompt.png" title="Advancing Textual Prompt Learning with Anchored Attributes.">
        <div><strong>Advancing Textual Prompt Learning with Anchored Attributes.</strong>
            <br>Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li#, Jian Yang# <br>in ICCV, 2025<br>
            <a href="https://arxiv.org/abs/2412.09442">[Paper]</a>
            <a href="./resources/bibtex/ICCV_2025_ATPrompt.txt">[BibTex]</a>
            <a href="https://github.com/zhengli97/ATPrompt">[Code]</a><img src="https://img.shields.io/github/stars/zhengli97/ATPrompt?style=social"/>
            <a href="https://zhuanlan.zhihu.com/p/11787739769">[中文解读]</a>
            <a href="https://github.com/zhengli97/ATPrompt/blob/main/docs/ATPrompt_chinese_version.pdf">[中文版]</a>
            <br>
            <alert>
            ATPrompt introduces a new attribute-anchored prompt format that can be seamlessly integrated into existing textual prompt leraning methods and achieve general improvements.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>


	<div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2025_FGVTP.png" title="Fine-Grained Visual Text Prompting">
        <div><strong>Fine-Grained Visual Text Prompting</strong>
            <br>Lingfeng Yang, Xiang Li#, Yueze Wang, Xinlong Wang, Jian Yang#<br>in TPAMI, 2025<br>
            <a href="https://ieeexplore.ieee.org/document/10763465">[Paper]</a>
            <a href="./resources/bibtex/TPAMI_2025_FGVTP.bib">[BibTex]</a>
            <a href="https://github.com/ylingfeng/FGVP">[Code]</a><img src="https://img.shields.io/github/stars/ylingfeng/FGVP?style=social"/>
            <br>
            <alert>
            FGVTP is an improved fine-grained multimodal prompting method over FGVP, enhancing large multimodal models’ localization and grounding via consistent visual–textual alignment.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>

    <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2025_NAIP.png"
            title="From Words to Worth: Newborn Article Impact Prediction with LLM">
        <div><strong>From Words to Worth: Newborn Article Impact Prediction with LLM</strong>
            <br>Penghai Zhao, Qinghua Xing, Kairan Dou, Jinyu Tian, Ying Tai, Jian Yang, Ming-Ming Cheng, Xiang Li#<br>in AAAI, 2025<br>
            <a href="https://arxiv.org/abs/2408.03934">[Paper]</a>
            <a href="./resources/bibtex/AAAI_2025_NAIP.bib">[BibTex]</a>
            <a href="https://github.com/ssocean/NAIP">[Code]</a><img
                src="https://img.shields.io/github/stars/ssocean/NAIP?style=social" />
            <a href="https://zhuanlan.zhihu.com/p/9630622286">[中文解读]</a>
            <br>
            <alert>
                This paper introduces an LLM-based method to predict newborn article impact from titles and abstracts, supported by a
                new normalized indicator (TNCSI_SP) and a 12K curated dataset.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>


	<div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2024_SARDet.png"
				   title="Sardet-100k: Towards open-source benchmark and toolkit for large-scale sar object detection">
            <div><strong>Sardet-100k: Towards open-source benchmark and toolkit for large-scale sar object detection</strong><br>
		Yuxuan Li, Xiang Li#, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng, Jian Yang#, <br>
                in NeurIPS, 2024 (Spotlight🎈) <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/e7eb8128eb26eafbe901348df1dbacdc-Paper-Conference.pdf">[Paper]</a>
                <a href="./resources/bibtex/NeurIPS_2024_SARDet.txt">[BibTex]</a>
                <a href="https://github.com/zcablii/SARDet_100K">[Code]</a><img
                        src="https://img.shields.io/github/stars/zcablii/SARDet_100K?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/686785188">[中文解读]</a>
                <br>
                <alert> 
        SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. A SAR object detection pretrain method: Multi-Stage with Filter Augmentation (MSFA) is proposed to tackle the domain gap problems from the perspective of data input, domain transition, and model migration.
		</alert>
            </div>
            <div class="spanner"></div>
    </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2023_LSKNet.png"
				   title="LSKNet: A Foundation Lightweight Backbone for Remote Sensing">
            <div><strong>LSKNet: A Foundation Lightweight Backbone for Remote Sensing</strong><br>
		Yuxuan Li, Xiang Li#,  Yimian Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang#, <br>
                in IJCV, 2024<br>
                <a href="http://openaccess.thecvf.com/content/ICCV2023/papers/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.pdf">[Paper]</a>
                <a href="./resources/bibtex/IJCV_2024_LSKNet.txt">[BibTex]</a>
                <a href="https://github.com/zcablii/LSKNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/zcablii/LSKNet?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/614449075">[中文解读]</a>
                <br>
                <alert> 
        LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various categories of objects in remote sensing scenarios. The lightweight LSKNet backbone network sets new state-of-the-art scores on standard remote sensing classification, object detection, semantic segmentation and change detection benchmarks.
		</alert>
            </div>
            <div class="spanner"></div>
    </div>


    <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2024_PromptKD.png" title="PromptKD: Unsupervised Prompt Distillation for Vision-Language Models.">
        <div><strong>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models.</strong>
            <br>Zheng Li, Xiang Li#, Xinyi Fu, Xin Zhang, Weiqiang Wang, Shuo Chen, Jian Yang#.<br>in CVPR, 2024<br>
            <a href="https://arxiv.org/abs/2403.02781">[Paper]</a>
            <a href="./resources/bibtex/CVPR_2024.PromptKD.txt">[BibTex]</a>
            <a href="https://github.com/zhengli97/PromptKD">[Code]</a><img src="https://img.shields.io/github/stars/zhengli97/PromptKD?style=social"/>
            <a href="https://zhuanlan.zhihu.com/p/684269963">[中文解读]</a>
            <a href="https://github.com/zhengli97/PromptKD/blob/main/docs/PromptKD_chinese_version.pdf">[中文版]</a>
            <a href="https://www.techbeat.net/talk-info?id=915">[中文视频]</a>
            <br>
            <alert>
            PromptKD is a simple and effective prompt-driven unsupervised distillation framework for VLMs (e.g., CLIP), with state-of-the-art performance.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2023_FGVP.png" title="Fine-Grained Visual Prompting">
        <div><strong>Fine-Grained Visual Prompting</strong>
            <br>Lingfeng Yang, Yueze Wang, Xiang Li#, Xinlong Wang, Jian Yang#<br>in NeurIPS, 2023<br>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4e9fa6e716940a7cfc60c46e6f702f52-Paper-Conference.pdf">[Paper]</a>
            <a href="./resources/bibtex/NeurIPS-2023-fine-grained-visual-prompting-Bibtex.bib">[BibTex]</a>
            <a href="https://github.com/ylingfeng/FGVP">[Code]</a><img src="https://img.shields.io/github/stars/ylingfeng/FGVP?style=social"/>
            <a href="https://mp.weixin.qq.com/s?search_click_id=10536340093298438394-1705732863737-1260009527&__biz=MzUxMDE4MzAzOA==&mid=2247714099&idx=1&sn=efe4d92ccece149d624d44a19f75404f&chksm=f8982f6663c6f4103967040294490fb7419803ceb6b54f2e79de728104a1858ad03f011d3fb8&scene=7&subscene=90&sessionid=1705732839&clicktime=1705732863&enterid=1705732863&ascene=65&fasttmpl_type=0&fasttmpl_fullversion=7038836-zh_CN-zip&fasttmpl_flag=0&realreporttime=1705732863790&devicetype=android-33&version=28002d3b&nettype=WIFI&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&countrycode=CN&exportkey=n_ChQIAhIQTY3OsEwNdtlJy0RxUEMZyxLcAQIE97dBBAEAAAAAAJ%2F5F8UMLd0AAAAOpnltbLcz9gKNyK89dVj0fDJfc0iQOozTOSv7wroTFtyx6pfMLQW9ACiiUD2XPYTJToJQxVNxvrF5tAIC8R0SbOS35hwJULATy64LUtXxEgmsCoz6Cqv01v%2B25HzaDWybt6vi82M5Lad5HaUdHZAgh4kTKQl9Lri9nQxeptfavWT7F389xOk%2BXh7B4nHuFz%2BeaRdMmZf6lLv3kLpf10%2BJykklCd3SfLyGkE68DPfh1hmFhext2v%2BZTOids%2B0QavnzY7GPOQE%3D&pass_ticket=h3SZ5GzwbdiBvmS547xoTsCldqEAFLvligHaiMY%2BXuAaSiUHNNO2iFTVImHJqOpfAucoZ0LcWe34Hs99pbaVbA%3D%3D&wx_header=3&poc_token=HEYkVWijKQwOws52LqNI8BFkPicAMjsAOeCl7vHt">[中文解读]</a>
            <a href="https://www.bilibili.com/video/BV1qw411873s/?spm_id_from=333.999.0.0&vd_source=55bfc02adba971ea9a2c7d47e95180cc">[中文视频]</a>
            <br>
            <alert>
            FGVP is a visual prompting technique that improves referring expression comprehension by highlighting regions of interest via fine-grained segmentation, achieving better accuracy with faster inference than state-of-the-art methods.
            </alert>
        </div>
        <div class="spanner"></div>
    </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2023_LSKNet.png"
				   title="Large Selective Kernel Network for Remote Sensing Object Detection">
            <div><strong>Large Selective Kernel Network for Remote Sensing Object Detection</strong><br>
		Yuxuan Li, Qibin Hou, Zhaohui Zheng, Ming-Ming Cheng, Jian Yang#, Xiang Li#<br>
                in ICCV, 2023<br>
                <a href="https://arxiv.org/pdf/2303.09030.pdf">[Paper]</a>
                <a href="./resources/bibtex/ICCV_2023_LSKNet.txt">[BibTex]</a>
                <a href="https://github.com/zcablii/LSKNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/zcablii/LSKNet?style=social"/>
                <br>
                <alert>
        LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various categories of objects in remote sensing scenarios.
		</alert>
            </div>
            <div class="spanner"></div>
    </div>
	
	<div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2023_CTKD.png"
	     title="Curriculum Temperature for Knowledge Distillation">
            <div><strong>Curriculum Temperature for Knowledge Distillation</strong><br>
		Zheng Li, <strong>Xiang Li</strong>#, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, Jian Yang#<br>
                in AAAI, 2023<br>
                <a href="https://arxiv.org/pdf/2211.16231.pdf">[Paper]</a>
                <a href="./resources/bibtex/AAAI_2023_CTKD.txt">[BibTex]</a>
                <a href="https://github.com/zhengli97/CTKD">[Code]</a><img src="https://img.shields.io/github/stars/zhengli97/CTKD?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/595735843">[中文解读]</a>
                <br>
                <alert>
        CTKD organizes the distillation task from easy to hard through a dynamic and learnable temperature. The temperature is learned during the student’s training process with a reversed gradient that aims to maximize the distillation loss in an adversarial manner.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2022_RM.png"
	     title="RecursiveMix: Mixed Learning with History">
            <div><strong>RecursiveMix: Mixed Learning with History</strong><br>
		Lingfeng Yang*, <strong>Xiang Li</strong>*, Borui Zhao, Renjie Song, Jian Yang#<br>
                in NeurIPS (<strong>Spotlight</strong>), 2022<br>
                <a href="https://openreview.net/pdf?id=NjP18IbKKlX">[Paper]</a>
                <a href="./resources/bibtex/NeurIPS_2022_RM.txt">[BibTex]</a>
                <a href="https://github.com/implus/RecursiveMix-pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/RecursiveMix-pytorch?style=social"/>
                <br>
                <alert>
        RecursiveMix is a simple but effective data augmentation technique that first leverages the historical input-prediction-label triplets.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2022_DTG.png"
	     title="DTG-SSOD: Dense Teacher Guidance for Semi-Supervised Object Detection">
            <div><strong>DTG-SSOD: Dense Teacher Guidance for Semi-Supervised Object Detection</strong><br>
		Gang Li, <strong>Xiang Li</strong>#, Yujie Wang, Yichao Wu, Ding Liang, Shanshan Zhang#<br>
                in NeurIPS, 2022<br>
                <a href="https://openreview.net/pdf?id=0-uBrFiOVf">[Paper]</a>
                <a href="./resources/bibtex/NeurIPS_2022_DTG.txt">[BibTex]</a>
                <a href="https://github.com/ligang-cs/DTG-SSOD">[Code(to be released)]</a><img
                        src="https://img.shields.io/github/stars/ligang-cs/DTG-SSOD?style=social"/>
                <br>
                <alert>
        DTG-SSOD explores a novel “dense-to-dense” paradigm, instead of the traditional  “sparse-to-dense” paradigm, for effective semi-supervised object detection.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/ECCV_2022_PseCo.png"
	     title="PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection">
            <div><strong>PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection</strong><br>
		Gang Li, <strong>Xiang Li</strong>#, Yujie Wang, Yichao Wu, Ding Liang, Shanshan Zhang#<br>
                in ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2203.16317.pdf">[Paper]</a>
                <a href="./resources/bibtex/ECCV_2022_PseCo.txt">[BibTex]</a>
                <a href="https://github.com/ligang-cs/PseCo">[Code]</a><img
                        src="https://img.shields.io/github/stars/ligang-cs/PseCo?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/544346080">[Blog(Chinese)]</a>
		<a href="https://www.bilibili.com/video/BV1DP411j7kg/">[Video(Chinese)]</a>
                <br>
                <alert>
        PseCo delves into two key techniques of semi-supervised learning (e.g., pseudo labeling and consistency training) for SSOD, and integrate object detection properties into them. 
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2022_UMMAE.png"
	     title="Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality">
            <div><strong>Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality</strong><br>
		<strong>Xiang Li</strong>, Wenhai Wang, Lingfeng Yang, Jian Yang#<br>
                in arXiv, 2022<br>
                <a href="https://arxiv.org/pdf/2205.10063.pdf">[Paper]</a>
                <a href="./resources/bibtex/arXiv_2022_UMMAE.txt">[BibTex]</a>
                <a href="https://github.com/implus/UM-MAE">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/UM-MAE?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/520228061">[Blog(Chinese)]</a>
                <br>
                <alert>
        UM-MAE is an efficient and general technique that supports MAE-style MIM Pre-training for popular Pyramid-based Vision Transformers (e.g., PVT, Swin).
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2022_DynamicMLP.png"
	     title="Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information">
            <div><strong>Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information</strong><br>
		Lingfeng Yang, <strong>Xiang Li</strong>#, Renjie Song, Borui Zhao, Juntian Tao, Shihao Zhou, Jiajun Liang, Jian Yang#<br>
                in CVPR (<strong>Oral</strong>), 2022<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="https://arxiv.org/pdf/2203.03253.pdf">[Paper]</a>
                <a href="./resources/bibtex/dynamicMLP.txt">[BibTex]</a>
                <a href="https://github.com/ylingfeng/DynamicMLP">[Code]</a><img
                        src="https://img.shields.io/github/stars/ylingfeng/DynamicMLP?style=social"/>
                <br>
                <alert>
		A very simple and effective approach for fine-grained recognition tasks using auxiliary knowledge like geographical/temporal information. We achieve SOTA results and take third place in the iNaturalist challenge at FGVC8 (CVPR21 workshop)
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_KD.png"
            title="Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation">
            <div><strong>Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation</strong><br>
		Gang Li*, <strong>Xiang Li</strong>*, Yujie Wang, Shanshan Zhang#, Yichao Wu, Ding Liang
                in AAAI, 2022<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="https://arxiv.org/pdf/2112.04840.pdf">[Paper]</a>
                <a href="./resources/bibtex/kd_rm_pfi.txt">[BibTex]</a>
                <br>
                <alert>Rank Mimicking and Prediction-guided Feature Imitation for knowledge Distillation of Dense Object Detection, A Simple and Effective Approach!</alert>
            </div>
            <div class="spanner"></div>
        </div>
	

	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2021_PVTv2.png"
            title="PVTv2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVTv2: Improved Baselines with Pyramid Vision Transformer</strong><br>
		Wenhai Wang, Enze Xie, <strong>Xiang Li</strong>, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
                Luo, Ling Shao<br>
                Technical Report, 2021<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="./resources/reports/PVTv2_Improved_Baselines_with_Pyramid_Vision_Transformer.pdf">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                    src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="./resources/bibtex/arXiv_2021_PVTv2.txt">[BibTex]</a>
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2021_PVT.png"
                                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong><br>
	    	Wenhai Wang, Enze Xie, <strong>Xiang Li</strong>, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#, Ping
                Luo, Ling Shao<br>
                in ICCV, 2021 (<strong>oral presentation</strong>)<br>
                <a href="https://arxiv.org/pdf/2102.12122.pdf">[Paper]</a>
                <!--                [<a href="./resources/posters/ICCV_2019_PAN.pdf">Poster</a>]-->
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="./resources/bibtex/ICCV_2021_PVT.txt">[BibTex]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2021_PAN++.png"
            title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text</strong><br>
	    	Wenhai Wang*, Enze Xie*, <strong>Xiang Li</strong>, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#, Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/>
                <a href="./resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2021_GFLv2.png"
                                title="Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection">
            <div><strong>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object
                Detection</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in CVPR, 2021<br>
                <a href="https://arxiv.org/pdf/2011.12885.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocalV2">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/GFocalV2?style=social"/>
                <a href="./resources/bibtex/CVPR_2021_GFLv2.txt">[BibTex]</a>
                <br>
                <alert>The improved version of GFocal!
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/GFocal.png"
                                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in NeurIPS, 2020<br>
                [<a href="https://arxiv.org/pdf/2006.04388.pdf">Paper</a>]
                [<a href="https://github.com/implus/GFocal">Code</a>]<img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense object detector. GFocal is 
                officially included in [<a href="https://github.com/open-mmlab/mmdetection">MMDetection</a>], and is an important part of the
                [<a href="https://dy.163.com/article/FLF2LGTP0511ABV6.html">winning solution</a>] in GigaVision contest (object detection and tracking tracks)
                hosted in ECCV 2020 workshop (winner: DeepBlueAI team).
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_SKNet.png"
                                title="Selective kernel networks">
            <div><strong>Selective kernel networks</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_SKNet.txt">BibTex</a>]
                [<a href="https://github.com/implus/PytorchInsight">Code</a>]<img src="https://img.shields.io/github/stars/implus/PytorchInsight?style=social"/>
                <br>
                <alert>We propose a selective kernel mechanism for convolution.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_DIS_DROPOUT_BN.png"
                                title="Understanding the disharmony between dropout and batch normalization by variance shift">
            <div><strong>Understanding the disharmony between dropout and batch normalization by variance shift</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_DIS_DROPOUT_BN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between dropout and batch normalization.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2020_DIS_WN_WD.png"
                                title="Understanding the disharmony between weight normalization family and weight decay">
            <div><strong>Understanding the disharmony between weight normalization family and weight decay</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Jian Yang<br>
                in AAAI, 2020<br>
                [<a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiX.1379.pdf">Paper</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between weight normalization family and weight decay.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2016_LightRNN.png"
                                title="LightRNN: Memory and computation-efficient recurrent neural networks">
            <div><strong>LightRNN: Memory and computation-efficient recurrent neural networks</strong><br>
                <strong>Xiang Li</strong>*, Tao Qin, Jian Yang, Tie-Yan Liu<br>
                in NeurIPS, 2016<br>
                [<a href="https://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks.pdf">Paper</a>]
                [<a href="./resources/bibtex/NeurIPS_2016_LightRNN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We propose a memory and computation-efficient recurrent neural networks for language model.</alert>
            </div>
            <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2018_STCGAN.png"
                                title="Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">
            <div><strong>Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal</strong><br>
                Jifeng Wang*, <strong>Xiang Li</strong>*, Jian Yang<br>
                in CVPR, 2018<br>
                [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2018_STCGAN.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/ST-CGAN">Dataset</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/ST-CGAN?style=social"/>
                <br>
                <alert>We release a new dataset for jointly shadow detection and removal.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_PSENet.png"
                                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                Wenhai Wang*, Enze Xie*, <strong>Xiang Li</strong>*, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/posters/CVPR_2019_PSENet.pdf">Poster</a>]
                [<a href="./resources/bibtex/CVPR_2019_PSENet.txt">BibTex</a>]
                [<a href="https://github.com/whai362/PSENet">Code</a>]<img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2018_MixNet.png"
                                title="Mixed Link Networks">
            <div><strong>Mixed Link Networks</strong><br>
                Wenhai Wang*, <strong>Xiang Li</strong>*, Jian Yang, Tong Lu<br>
                in IJCAI, 2018<br>
                [<a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">Paper</a>]
                [<a href="./resources/posters/IJCAI_2018_MixNet.pdf">Poster</a>]
                [<a href="./resources/bibtex/IJCAI_2018_MixNet.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/MixNet">Code</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/MixNet?style=social"/>
                <br>
                <alert>We proposed an parameter-efficient convolutional neural networks for image classification. </alert>
            </div>
            <div class="spanner"></div>
        </div>

    </div>
</div>





<div style="clear: both;">
    <div class="section">
        <h2 id="review">Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
	    IEEE Transactions on Neural Networks and Learning Systems (TNNLS)<br>
	    IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
	    International Journal of Computer Vision (IJCV)<br>
	    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
            <br>

            <strong>Conference Reviewer</strong><br>
	    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022, 2023<br>
	    AAAI Conference on Artificial Intelligence (AAAI), 2019, 2020, 2021, 2022, 2023<br>
	    European Conference on Computer Vision (ECCV), 2022<br>
        </div>
    </div>
</div>

<div style='width:600px;height:300px;margin:0 auto'>
    <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&cl=ffffff&w=a"></script>-->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
</div>

<!-- 图片查看模态框 -->
<div id="imageModal" class="modal">
    <span class="close">&times;</span>
    <div class="modal-content">
        <img id="modalImg" class="modal-img">
        <div class="zoom-controls">
            <button class="zoom-btn" id="zoomOut">-</button>
            <button class="zoom-btn" id="zoomReset">重置</button>
            <button class="zoom-btn" id="zoomIn">+</button>
        </div>
    </div>
</div>

<script>
    // 获取模态框元素
    const modal = document.getElementById("imageModal");
    const modalImg = document.getElementById("modalImg");
    const closeBtn = document.getElementsByClassName("close")[0];
    const zoomIn = document.getElementById("zoomIn");
    const zoomOut = document.getElementById("zoomOut");
    const zoomReset = document.getElementById("zoomReset");
    
    // 当前缩放比例
    let currentZoom = 1;
    
    // 为Honors Gallery中的所有图片添加点击事件
    document.addEventListener('DOMContentLoaded', function() {
        // 获取Honors Gallery部分的所有图片
        const gallerySection = document.getElementById('honors_gallery').closest('.section');
        const images = gallerySection.querySelectorAll('img');
        
        // 为每个图片添加点击事件和cursor样式
        images.forEach(img => {
            img.classList.add('honors-gallery-img');
            img.onclick = function() {
                modal.style.display = "flex";
                modalImg.src = this.src;
                currentZoom = 1;
                modalImg.style.transform = `scale(${currentZoom})`;
                document.body.style.overflow = "hidden"; // 防止背景滚动
            }
        });
    });
    
    // 点击关闭按钮关闭模态框
    closeBtn.onclick = function() {
        closeModal();
    }
    
    // 点击模态框背景关闭模态框
    modal.onclick = function(event) {
        if (event.target === modal) {
            closeModal();
        }
    }
    
    // 关闭模态框函数
    function closeModal() {
        modal.style.display = "none";
        document.body.style.overflow = "auto"; // 恢复背景滚动
    }
    
    // 缩放控制
    zoomIn.onclick = function(e) {
        e.stopPropagation();
        currentZoom += 0.1;
        modalImg.style.transform = `scale(${currentZoom})`;
    }
    
    zoomOut.onclick = function(e) {
        e.stopPropagation();
        if (currentZoom > 0.5) {
            currentZoom -= 0.1;
            modalImg.style.transform = `scale(${currentZoom})`;
        }
    }
    
    zoomReset.onclick = function(e) {
        e.stopPropagation();
        currentZoom = 1;
        modalImg.style.transform = `scale(${currentZoom})`;
    }
    
    // 键盘事件处理
    document.addEventListener('keydown', function(e) {
        if (modal.style.display === "flex") {
            if (e.key === "Escape") {
                closeModal();
            } else if (e.key === "+" || e.key === "=") {
                currentZoom += 0.1;
                modalImg.style.transform = `scale(${currentZoom})`;
            } else if (e.key === "-" && currentZoom > 0.5) {
                currentZoom -= 0.1;
                modalImg.style.transform = `scale(${currentZoom})`;
            } else if (e.key === "0") {
                currentZoom = 1;
                modalImg.style.transform = `scale(${currentZoom})`;
            }
        }
    });
</script>

</body>
</html>
