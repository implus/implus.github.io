<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiang Li</title>
    <meta content="Xiang Li, https://implus.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/me.png">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Xiang Li (李翔)</span><br>
            <span> <a href="https://cc.nankai.edu.cn/2022/0908/c13620a473455/page.htm">Associate Professor, College of Computer Science, Nankai University</a></span><br>
            <span><strong>Address</strong>: No. 38, Tongyan Road, Haihe Education Park, Tianjin, China</span><br>
            <span><strong>Email</strong>: xiang.li.implus [at] {nankai.edu.cn} </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me [<a href="https://github.com/implus">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ&hl=zh-CN">Google Scholar</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->
        </h2>
        <div class="paper">
            I'm an <a href="https://cc.nankai.edu.cn/2022/0908/c13620a473455/page.htm">Associate Professor</a> in College of Computer Science, Nankai University. 
            I got my PhD degree from the Department of Computer Science and Technology, Nanjing University of Science and Technology (NJUST) in 2020.
            I started my postdoctoral career in NJUST as a candidate for the <a href="https://zhuanlan.zhihu.com/p/147471409">2020 Postdoctoral Innovative Talent Program</a>.
            My advisor is <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Prof. Jian Yang</a> from NJUST, who is a Changjiang Scholar. My vice-advisor is <a href="http://www.xlhu.cn/">Prof. Xiaolin Hu</a> from Tsinghua University.
            In 2016, I spent 8 months as a research intern in Microsoft Research Asia, supervised by <a href="https://scholar.google.com/citations?user=Bl4SRU0AAAAJ&hl=zh-CN">Prof. Tao Qin</a> and <a href="https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=zh-CN">Prof. Tie-Yan Liu</a>.
            I was a visiting scholar at <a href="https://www.momenta.cn/">Momenta</a>, mainly focusing on monocular perception algorithm.
            <br><br>

            My recent works are mainly on:
	    <ul>
		<li>neural architecture design, CNN/Transformer</li>
		<li>object detection/recognition</li>
		<li>unsupervised learning</li>
		<li>knowledge distillation</li>
	    </ul>
            <p style='color:red'><strong>
                 We are looking for self-motivated PhD candidates! Please feel free to contact me through the email (attach your CV, preferably with a CCF-A paper).
		 During the PhD career, you can have:
		 <ul> 
			<li>joint supervision with well-known research institute (e.g., Megvii, SenseTime Research, Huawei Noah's Ark Lab, BAAI)</li>
			<li>hand in hand guidance to publish earlier papers</li>
			<li>relatively flexible and free research space</li>
		 </ul>
		 We would not push hard, but you should always be self-driven for your own target, i.e., making solid and impactful contributions to the CV/AI community.
            </strong></p> 
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
                <li>
                    Second place of 2020 Zhengtu Cup's first AI competition, namely the industrial defect detection algorithm, <strong>150,000 RMB bonus (2st from 900 teams)</strong>
                </li>
                <li>
                    Champion of 2016 Didi Tech Di-Tech's first big data competition, namely the travel demand prediction algorithm, <strong>100,000 US dollars bonus (1st from 7664 team)</strong>
                </li>
                <li>
                    Champion of 2015 Alibaba Tianchi's first big data competition, namely Ali mobile recommendation algorithm, <strong>300,000 RMB bonus (1st from 7186 team)</strong> 
                </li>
                <li>
                    2015 Dean Medal of School of Computer Science, Nanjing University of Science and Technology, 2016 Presidential Medal of Nanjing University of Science and Technology, 2016 National Scholarship
                </li>
                <li>
                    ACM-ICPC Asia Regional Contest, Silver Medal (1st)
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
		<li> 
			2022-09-15: 2 paper (<a href="https://arxiv.org/pdf/2203.06844.pdf">RecursiveMix</a>, <a href="https://arxiv.org/pdf/2207.05536.pdf">DTG-SSOD</a> accepted in NeurIPS 2022. 
		</li>
	     	<li> 
			2022-07-05: 3 paper (<a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet</a>, <a href="https://arxiv.org/pdf/2203.09855.pdf">M3PT</a>, <a href="https://arxiv.org/pdf/2203.16317.pdf">PseCo</a>) accepted in ECCV 2022. 
		</li>
	     	<li> 
			2022-05-20: 1 paper (<a href="https://arxiv.org/pdf/2205.10063.pdf">UM-MAE</a>) is publicly available in arXiv. 
		</li>
	     	<li> 
			2022-03-02: 1 paper (<a href="https://arxiv.org/pdf/2203.03253.pdf">dynamicMLP</a>) accepted (oral) in CVPR 2022. 
		</li>
	     	<li> 
			2021-12-01: 1 paper (<a href="https://arxiv.org/pdf/2112.04840.pdf">KD for object detection</a>) accepted in AAAI 2022. 
		</li>
		<li>
                    	2021-05-05: 1 paper (<a href="https://ieeexplore.ieee.org/document/9423611">PAN++</a>) is accepted by TPAMI 2021. 
                </li>
                <li> 
			2021-03-01: 1 paper (<a href="https://arxiv.org/pdf/2011.12885.pdf">GFocalv2</a>) accepted in CVPR 2021.  
		</li>
                <li> 	
			2020-09-25: 1 paper (<a href="https://arxiv.org/pdf/2006.04388.pdf">GFocal</a>) accepted in NeurIPS 2020.  
		</li>
                <li> 	2019-12-01: 1 paper (<a href="https://ojs.aaai.org/index.php/AAAI/article/download/5904/5760">Understanding the disharmony v2</a>) accepted in AAAI 2020.  </li>
                <li> 	2019-03-15: 3 papers (<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">SKNet</a>,
			<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">Understanding the disharmony v1</a>,
			<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">PSENet</a>) accepted in CVPR 2019. </li>
		<li> 	2017-03-01: 1 paper (<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf">ST-CGAN</a>) accepted in CVPR 2018.  </li>
                <li> 	2018-06-16: 1 paper (<a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">MixNet</a>) accepted in IJCAI 2018. </li>
		<li>    2016-09-30: 1 paper (<a href="https://arxiv.org/pdf/1610.09893.pdf">LightRNN</a>) accepted in NeurIPS 2016. </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Selected Publications</h2>
	(* indicates equal contribution, # corresponding author)
	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2022_RM.png"
	     title="RecursiveMix: Mixed Learning with History">
            <div><strong>RecursiveMix: Mixed Learning with History</strong><br>
		Lingfeng Yang*, <strong>Xiang Li</strong>*, Borui Zhao, Renjie Song, Jian Yang#<br>
                in NeurIPS, 2022<br>
                <a href="https://arxiv.org/pdf/2203.06844.pdf">[Paper]</a>
                <a href="./resources/bibtex/arXiv_2022_RM.txt">[BibTex]</a>
                <a href="https://github.com/implus/RecursiveMix-pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/RecursiveMix-pytorch?style=social"/>
                <br>
                <alert>
        RecursiveMix is a simple but effective data augmentation technique that first leverages the historical input-prediction-label triplets.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2022_PseCo.png"
	     title="PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection">
            <div><strong>PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection</strong><br>
		Gang Li, <strong>Xiang Li</strong>#, Yujie Wang, Yichao Wu, Ding Liang, Shanshan Zhang#<br>
                in ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2203.16317.pdf">[Paper]</a>
                <a href="./resources/bibtex/arXiv_2022_PseCo.txt">[BibTex]</a>
                <a href="https://github.com/ligang-cs/PseCo">[Code]</a><img
                        src="https://img.shields.io/github/stars/ligang-cs/PseCo?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/544346080">[Blog(Chinese)]</a>
		<a href="https://www.bilibili.com/video/BV1DP411j7kg/">[Video(Chinese)]</a>
                <br>
                <alert>
        PseCo delves into two key techniques of semi-supervised learning (e.g., pseudo labeling and consistency training) for SSOD, and integrate object detection properties into them. 
		</alert>
            </div>
            <div class="spanner"></div>
        </div>
	    
	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2022_UMMAE.png"
	     title="Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality">
            <div><strong>Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality</strong><br>
		<strong>Xiang Li</strong>, Wenhai Wang, Lingfeng Yang, Jian Yang#<br>
                in arXiv, 2022<br>
                <a href="https://arxiv.org/pdf/2205.10063.pdf">[Paper]</a>
                <a href="./resources/bibtex/arXiv_2022_UMMAE.txt">[BibTex]</a>
                <a href="https://github.com/implus/UM-MAE">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/UM-MAE?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/520228061">[Blog(Chinese)]</a>
                <br>
                <alert>
        UM-MAE is an efficient and general technique that supports MAE-style MIM Pre-training for popular Pyramid-based Vision Transformers (e.g., PVT, Swin).
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2022_DynamicMLP.png"
	     title="Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information">
            <div><strong>Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information</strong><br>
		Lingfeng Yang, <strong>Xiang Li</strong>#, Renjie Song, Borui Zhao, Juntian Tao, Shihao Zhou, Jiajun Liang, Jian Yang#<br>
                in CVPR (oral), 2022<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="https://arxiv.org/pdf/2203.03253.pdf">[Paper]</a>
                <a href="./resources/bibtex/dynamicMLP.txt">[BibTex]</a>
                <a href="https://github.com/ylingfeng/DynamicMLP">[Code]</a>
                <br>
                <alert>
		A very simple and effective approach for fine-grained recognition tasks using auxiliary knowledge like geographical/temporal information. We achieve SOTA results and take third place in the iNaturalist challenge at FGVC8 (CVPR21 workshop)
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_KD.png"
            title="Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation">
            <div><strong>Knowledge Distillation for Object Detection via Rank Mimicking and Prediction-guided Feature Imitation</strong><br>
		Gang Li*, <strong>Xiang Li</strong>*, Yujie Wang, Shanshan Zhang#, Yichao Wu, Ding Liang
                in AAAI, 2022<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="https://arxiv.org/pdf/2112.04840.pdf">[Paper]</a>
                <a href="./resources/bibtex/kd_rm_pfi.txt">[BibTex]</a>
                <br>
                <alert>Rank Mimicking and Prediction-guided Feature Imitation for knowledge Distillation of Dense Object Detection, A Simple and Effective Approach!</alert>
            </div>
            <div class="spanner"></div>
        </div>
	

	<div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2021_PVTv2.png"
            title="PVTv2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVTv2: Improved Baselines with Pyramid Vision Transformer</strong><br>
		Wenhai Wang, Enze Xie, <strong>Xiang Li</strong>, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
                Luo, Ling Shao<br>
                Technical Report, 2021<br>
                <!-- <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a> -->
                <a href="./resources/reports/PVTv2_Improved_Baselines_with_Pyramid_Vision_Transformer.pdf">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                    src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="./resources/bibtex/arXiv_2021_PVTv2.txt">[BibTex]</a>
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2021_PVT.png"
                                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong><br>
	    	Wenhai Wang, Enze Xie, <strong>Xiang Li</strong>, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#, Ping
                Luo, Ling Shao<br>
                in ICCV, 2021 (<strong>oral presentation</strong>)<br>
                <a href="https://arxiv.org/pdf/2102.12122.pdf">[Paper]</a>
                <!--                [<a href="./resources/posters/ICCV_2019_PAN.pdf">Poster</a>]-->
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="./resources/bibtex/ICCV_2021_PVT.txt">[BibTex]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2021_PAN++.png"
            title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text</strong><br>
	    	Wenhai Wang*, Enze Xie*, <strong>Xiang Li</strong>, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#, Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/>
                <a href="./resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2021_GFLv2.png"
                                title="Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection">
            <div><strong>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object
                Detection</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in CVPR, 2021<br>
                <a href="https://arxiv.org/pdf/2011.12885.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocalV2">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/GFocalV2?style=social"/>
                <a href="./resources/bibtex/CVPR_2021_GFLv2.txt">[BibTex]</a>
                <br>
                <alert>The improved version of GFocal!
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/GFocal.png"
                                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in NeurIPS, 2020<br>
                [<a href="https://arxiv.org/pdf/2006.04388.pdf">Paper</a>]
                [<a href="https://github.com/implus/GFocal">Code</a>]<img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense object detector. GFocal is 
                officially included in [<a href="https://github.com/open-mmlab/mmdetection">MMDetection</a>], and is an important part of the
                [<a href="https://dy.163.com/article/FLF2LGTP0511ABV6.html">winning solution</a>] in GigaVision contest (object detection and tracking tracks)
                hosted in ECCV 2020 workshop (winner: DeepBlueAI team).
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_SKNet.png"
                                title="Selective kernel networks">
            <div><strong>Selective kernel networks</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_SKNet.txt">BibTex</a>]
                [<a href="https://github.com/implus/PytorchInsight">Code</a>]<img src="https://img.shields.io/github/stars/implus/PytorchInsight?style=social"/>
                <br>
                <alert>We propose a selective kernel mechanism for convolution.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_DIS_DROPOUT_BN.png"
                                title="Understanding the disharmony between dropout and batch normalization by variance shift">
            <div><strong>Understanding the disharmony between dropout and batch normalization by variance shift</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_DIS_DROPOUT_BN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between dropout and batch normalization.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2020_DIS_WN_WD.png"
                                title="Understanding the disharmony between weight normalization family and weight decay">
            <div><strong>Understanding the disharmony between weight normalization family and weight decay</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Jian Yang<br>
                in AAAI, 2020<br>
                [<a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiX.1379.pdf">Paper</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between weight normalization family and weight decay.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2016_LightRNN.png"
                                title="LightRNN: Memory and computation-efficient recurrent neural networks">
            <div><strong>LightRNN: Memory and computation-efficient recurrent neural networks</strong><br>
                <strong>Xiang Li</strong>*, Tao Qin, Jian Yang, Tie-Yan Liu<br>
                in NeurIPS, 2016<br>
                [<a href="https://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks.pdf">Paper</a>]
                [<a href="./resources/bibtex/NeurIPS_2016_LightRNN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We propose a memory and computation-efficient recurrent neural networks for language model.</alert>
            </div>
            <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2018_STCGAN.png"
                                title="Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">
            <div><strong>Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal</strong><br>
                Jifeng Wang*, <strong>Xiang Li</strong>*, Jian Yang<br>
                in CVPR, 2018<br>
                [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2018_STCGAN.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/ST-CGAN">Dataset</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/ST-CGAN?style=social"/>
                <br>
                <alert>We release a new dataset for jointly shadow detection and removal.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_PSENet.png"
                                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                Wenhai Wang*, Enze Xie*, <strong>Xiang Li</strong>*, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/posters/CVPR_2019_PSENet.pdf">Poster</a>]
                [<a href="./resources/bibtex/CVPR_2019_PSENet.txt">BibTex</a>]
                [<a href="https://github.com/whai362/PSENet">Code</a>]<img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2018_MixNet.png"
                                title="Mixed Link Networks">
            <div><strong>Mixed Link Networks</strong><br>
                Wenhai Wang*, <strong>Xiang Li</strong>*, Jian Yang, Tong Lu<br>
                in IJCAI, 2018<br>
                [<a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">Paper</a>]
                [<a href="./resources/posters/IJCAI_2018_MixNet.pdf">Poster</a>]
                [<a href="./resources/bibtex/IJCAI_2018_MixNet.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/MixNet">Code</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/MixNet?style=social"/>
                <br>
                <alert>We proposed an parameter-efficient convolutional neural networks for image classification. </alert>
            </div>
            <div class="spanner"></div>
        </div>

    </div>
</div>



<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
	    IEEE Transactions on Neural Networks and Learning Systems (TNNLS)<br>
	    IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
            <br>

            <strong>Conference Reviewer</strong><br>
	    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022<br>
	    AAAI Conference on Artificial Intelligence (AAAI), 2019, 2020, 2021, 2022<br>
        </div>
    </div>
</div>

<div style='width:600px;height:300px;margin:0 auto'>
    <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&cl=ffffff&w=a"></script>-->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
</div>

</body>
</html>
